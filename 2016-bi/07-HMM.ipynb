{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Bioinformatics**<br>\n",
    "A masters course by Blaž Zupan and Tomaž Curk<br>\n",
    "University of Ljubljana, 2016\n",
    "\n",
    "Disclaimer: this work is a first draft of our working notes. The images were obtained from various web sites, but mainly from the wikipedia entries with explanations of different biological entities. Material is intended for our 2016 class and is not meant for distribution.\n",
    "\n",
    "## Lecture Notes Part 7\n",
    "\n",
    "# Introduction to Markov Chains, Hidden Markov Models and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a DNA sequence of a human genome 21 and let us do some nucleotide counting first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from Bio import SeqIO\n",
    "file_name = \"human21\"\n",
    "s = str(SeqIO.read(\"data/%s.fasta\" % file_name, \"fasta\").seq)\n",
    "\n",
    "def tuple_walk(s, k=2):\n",
    "    \"\"\"Generate subsequences of length k.\"\"\"\n",
    "    for i in range(len(s)-1):\n",
    "        yield s[i:i+k]\n",
    "\n",
    "from collections import Counter\n",
    "pairs = Counter(tuple_walk(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A': 9943435, 'C': 6864570, 'G': 6852178, 'N': 470, 'T': 9882679})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out adenine and thymine are in majority and are almost as frequent in the sequence, and presence of guanine and cytosine are is slightly less common. (You may wander what is N: it stands for [any nucleotide](https://en.wikipedia.org/wiki/Nucleic_acid_notation). Remember that the DNA sequence we have retrieved is a consensus sequence of a population of aligned sequences of different people). Let us observe the sequences of pairs of nucleotides that start with adenine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AA': 3312474, 'AC': 1706672, 'AG': 2314126, 'AN': 6, 'AT': 2610157}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{pair: pairs[pair] for pair in pairs if pair[0] == \"A\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect, under the assumption of a multinomial model, that the frequency of AA and AT would approximately match. But this is not so, and AA appears is more frequent.\n",
    "\n",
    "Even when we consider a simple property of a sequence, such as probabilities of pairs, multinomial model fails. And actually fails in a good way, where we can use it to show that there are patterns in the sequence which are due to chance. But we have been her in our previous lectures. Here, we would like to develop a better model, one with a more predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are generating a sequence, and that the choice for every new element of the sequence depends only on its immediate predecessor. We can represent this as a state graph, where the state encodes the last element of the sequence being generated, and the transitions denote probability of the next symbol in the sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two-state Markov model.](figs/07-two-state-model-b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes in this graph represent states. There are two states in our model, A and B. There is also a special state ${\\cal B}$ that denotes the starting point. Notice that in the model we would always start so that the first state, the first symbol of our sequence will be A. From A we traverse to B with probability of 0.6, or stay in A with probability of 0.4. Our model can generate any sequence of symbols A and B, but their probability will be different.\n",
    "\n",
    "For instance, the probability of a sequence AB is 0.6, as we start with a an A (probability of 1) and then traverse to state B with  probability (0.6). The probability of a sequence AAB is $0.4\\times 0.6=0.24$, as we start with A, then stay in A with probability of 0.4 and traverse to B with probability of 0.6.\n",
    "\n",
    "We will denote the sequence of states with $x=x_1 x_2\\ldots x_L$, and transition probabilities between states $s$ and $t$ with $a_{st}=P(x_i=t|x_{i-1}=s)$, which is also the conditional probability of $x_i=t$ given that the previous state $x_{i-1}$ is equal to $s$.\n",
    "\n",
    "Notice that the probability of $x_i$ depends only on the previous state, $x_{i-1}$. This is called a Markov property (from [Andrej Andrejevič Markov](https://en.wikipedia.org/wiki/Andrey_Markov), 1856-1922), and the sequences that are generated by the models with this property are called Markov chains.\n",
    "\n",
    "We can encode transition probabilities in a transition matrix, or, using a Python code, in a dictionary. Let us how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = {\"A\": {\"A\": 0.4, \"B\": 0.6}, \"B\": {\"A\": 0.4, \"B\": 0.6}}\n",
    "start = {\"A\": 1, \"B\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use this model to generate a sequence of lenght $L$. The following function that considers a dictionary with weighted items and uses weights to choose a random element according to its weight comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def weighted_choice(weighted_items):\n",
    "    \"\"\"Random choice given the list of elements and their weights\"\"\"\n",
    "    rnd = random.random() * sum(weighted_items.values())\n",
    "    for i, w in weighted_items.items():\n",
    "        rnd -= w\n",
    "        if rnd < 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us generate some sequences. First, we need to construct a sequence generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def markov_chain(start, model, length):\n",
    "    x = weighted_choice(start)\n",
    "    yield x\n",
    "    for _ in range(length-1):\n",
    "        yield weighted_choice(model[x[-1]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And then, our first check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(markov_chain(start, a, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few more sequences from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB\n",
      "ABABB\n",
      "AABAABBABBBAAAABABBA\n"
     ]
    }
   ],
   "source": [
    "ls = [2, 5, 20]\n",
    "for seq_len in ls:\n",
    "    print(\"\".join(markov_chain(start, a, seq_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we have already invented procedure for computing probability of a sequence. Here is a formal derivation. Note: we use Markov property, and definition of conditional probability where $P(X|Y)=P(X,Y)/P(Y)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{eqnarray} \n",
    "P(x) & = & P(x_1, x_2, \\ldots, x_L) \\\\\n",
    "& = & P(x_L, x{L-1}, \\ldots, x_1) \\\\\n",
    "& = & P(x_L|x_{L-1},\\ldots,x_1) P(x_{L-1}|x_{L-2},\\ldots,x_1) \\ldots P(x_1) \\\\\n",
    "& = & P(x_L|x_{L-1})P(x_{L-1}|x_{L-2})\\ldots P(x_2|x_1) P(x_1) \\\\\n",
    "& = & P(x_1)\\prod_{i=2}^L a_{x_{i-1,i}}\n",
    "\\end{eqnarray}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $P(x_1)$ is the probability of the start symbol. Let us see this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "import functools\n",
    "\n",
    "def prod(iterable):\n",
    "    \"\"\"Product of elements in an iterable\"\"\"\n",
    "    return functools.reduce(operator.mul, iterable, 1)\n",
    "\n",
    "def mc_prob(x):\n",
    "    p = start[x[0]]\n",
    "    return p * prod(a[x1][x2] for x1, x2 in tuple_walk(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_prob(\"AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_prob(\"AAB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.906778275839999e-06"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_prob(\"AABBABABABBBAAABBB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chains were just for warm-up. We are really looking for something a bit more complicated. The DNA sequences, namely, depend on the region of the genome. Say, the sequence will have different properties if it would be around the start of the gene, or within the coding region, or in the poly-A tail. When we sequence the genome, all we have is a sequence, and we do not know about regions. We could perhaps infer something about regions from the sequence. In the following, we will consider that the sequence is emitted from the model, and that the emissions depended on the state of the model, the region. The region, that is, the state, is unknown, or hidden. Hence the name, Hidden Markov Models or HMM.\n",
    "\n",
    "Let us start with example. There is magician around sitting at the street corner, throwing a coin and earning money from people that pass by who are called to guess the results, heads or tails. Somebody told us that this magician cheats (what else?) and uses two coins, where one is a fair coin and the other one is loaded (high probability of heads of 0.95 instead of 0.5 with fair coin). Magician switches the coins only rarely, with probability of 0.05. We are observing its throws, writing down the sequence. It would cool to know which type of the coin he is using and when does he switch.\n",
    "\n",
    "Here is the graphical depiction of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loaded coin model.](figs/07-loaded-coin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notation first:\n",
    "* $\\pi$ is called a path and is a sequence of hidden states, $\\pi_0\\pi_1\\ldots\\pi_L$. An example path is FFLFFLF.\n",
    "* $x$ is a sequence of emitted symbols. Because this is the sequence we can observe, we call it observed sequence. An example observed sequence from loaded coin model could be oo-o--.\n",
    "* $a_{kl}$ is a probability of transitions between hidden states $k$ and $l$, where $a_{kl}=P(\\pi_i=l|\\pi_{i-1}=k)$.\n",
    "* $a_{0k}$ is a probability of a starting state, taht is, probability that we start at the hidden state $k$.\n",
    "* $e_k(b)$ is probability of emitting symbol $b$ at hidden state $k$, that is $e_k(b)=P(x_i=b|\\pi_i=k)$. Parameters $e_k(b)$ therefore store emission probabilities.\n",
    "\n",
    "This is not too complicated: we have hidden states and emissions. The transitions between hidden states are defined with simple Markov chain model that uses transition probabilities. Observed sequence is composed of emissions that depend on the hidden state and are specified through emission probabilities defined for each of the hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us present the loaded dice model with some code. We specify the transition probabilities, emission probabilities (given the hidden state), and the start state (magician starts with a fair dice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = {\"F\": {\"F\": 0.95, \"L\": 0.05},\n",
    "     \"L\": {\"F\": 0.05, \"L\": 0.95}}\n",
    "E = {\"F\": {\"o\": 0.5, \"-\": 0.5},\n",
    "     \"L\": {\"o\": 0.7, \"-\": 0.3}}\n",
    "start = \"F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code that can receive HMM and uses it to generate a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_hmm_sequence(h, T, E, n):\n",
    "    \"\"\"\n",
    "    HMM sequence given start state,\n",
    "    transition, emission matrix and sequence length\n",
    "    \"\"\"\n",
    "    s = weighted_choice(E[h])\n",
    "    yield h, s\n",
    "    for _ in range(n-1):\n",
    "        h = weighted_choice(T[h])\n",
    "        yield h, weighted_choice(E[h])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try this out and generate a sequence of lenght 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden:  FFFFFFFFFFFFLLLLLLLLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFFFFFFFFLLLLLLLLLLLLL\n",
      "emitted: --oooooo---ooo-oo-ooooo-ooooooo----o-oo--o--o--o-o-o--o-ooooooo-oooo-o\n"
     ]
    }
   ],
   "source": [
    "path, emission = zip(*[i for i in generate_hmm_sequence(start, T, E, 70)])\n",
    "print(\"hidden: \", \"\".join(path))\n",
    "print(\"emitted:\", \"\".join(emission))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Probability of $\\pi$ and $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an observed sequence $x$ and a sequence of hidden states, we would like to compute their joint probability. Note this is still academic exercise, and preparation for what is to follow, since in practical applications the hidden states are, well, hidden. Nevertheless, it does not hurt to know their joint probability. Let us just write it down and then think about it:\n",
    "\n",
    "$$P(x,\\pi)=a_{0\\pi_1}\\prod_{i=1}^L e_{\\pi_i}(x_i)a_{\\pi_i \\pi_{i+1}}$$\n",
    "\n",
    "We start with the probability of the start symbol, and then traverse the sequence, multiplying the probability of transition of (hidden) states with probability of emitting a symbol at particular state. Simple, yet, again, not very useful as we do not know the hidden sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Another Example: Dishonest Casino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example: casino plays with two dices, one fair and one loaded. The loaded one has higher probability of turning 6, while probability for all the sides in the fair dice is the same (as it should be). Here is graphical depiction of the model and its representation in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loaded dice model.](figs/07-loaded-dice.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = A = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\n",
    "E = {\"F\": {a: 1/6. for a in A},\n",
    "     \"L\": {a: 1/10 if a != \"6\" else 0.5 for a in A}}\n",
    "T = {\"F\": {\"F\": 0.95, \"L\": 0.05},\n",
    "     \"L\": {\"F\": 0.1, \"L\": 0.9}}\n",
    "start = \"F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some sequences from this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden:  FFFFFFFFFFFFFFFFFLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLFFFFFFFFFFFFFFFFFFFFFFF\n",
      "emitted: 4135152121655511116562564624613666116645413636636442361514631455423523\n"
     ]
    }
   ],
   "source": [
    "path, emission = zip(*[i for i in generate_hmm_sequence(start, T, E, 70)])\n",
    "print(\"hidden: \", \"\".join(path))\n",
    "print(\"emitted:\", \"\".join(emission))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Probable State Path: The Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given is an observed sequence of emitted symbols $x$. Our task is to find the sequence of hidden states $\\pi^*$ with the highest joint probability $P(x,\\pi^*)$. That is,\n",
    "\n",
    "$$\\pi^*={\\rm argmax}_{\\pi} P(x,\\pi)$$\n",
    "\n",
    "This problem is solved by a dynamic programming algorithm called after [Andrea Viterbi](https://en.wikipedia.org/wiki/Andrew_Viterbi) (born in 1935 in Bergamo). Suppose that $v_k(i)$ is the probability of the most probable path ending in state $k$ with observation $x_i$. Then, the probability $v_l(i+1)$ is:\n",
    "\n",
    "$$ v_l(i+1) = e_l(x_{i+1})\\max_k(v_k(i) a_{kl}) $$\n",
    "\n",
    "Notice that $e_l(x_{i+1})$ is the probability of the emitted symbol $x_{i+1}$ in the state $l$. Notice also that this probability is defined since we know the emitted sequence and hence we know about the symbol $x_{i+1}$. To reach the state $l$ at instance $i+1$, we are therefore looking for the path from appropriate previous state $k$ that would maximize the joint probability. But since at instance $i$ the highest joint probability for that state was already computed as $v_k(i)$, probability $v_l(i+1)$ is the highest joint probability at instance $i+1$.\n",
    "\n",
    "We use Markov property to infer the maximal joint probabilities from the previous states, and previous states only.\n",
    "\n",
    "What Viterbi does is finding he most probable state path. It decodes the sequence of the hidden states based on the known sequence of observed states. For this reason, the procedure is also referred to as *decoding*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Viterbi Algorithm by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following hidden Markov model for loaded dice model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Another loaded dice model.](figs/07-viterbi-dice.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we added a special symbol for the transition to start state. We do not know if the magician will start with the fair or the loaded dice, so both states are equally likely start states.\n",
    "\n",
    "Say that the emitted sequence is:\n",
    "```\n",
    "oo--oooo-o\n",
    "```\n",
    "\n",
    "Now, let's do Viterbi algorithm. Eventually, this algorithm is dynamic programming algorithm, and hence we have initialization and construction of dynamic programming table. The initialization is this time shown in the first row of the table (the only probable state at the start of the procedure is ${\\cal B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_i$|${\\cal B}$|F|L|\n",
    "-|-|-|-|\n",
    "|$1.0$|$0.0$|$0.0$\n",
    "o||$.5\\max(1\\times .5,0)=0.25$|$.9\\max(1\\times .5,0)=.45$\n",
    "o||$.5\\max(.25\\times .8,.45\\times .1)=.1$|$.9\\max(.25\\times .2,.45\\times .9)=.364$\n",
    "-||$.5\\max(.1\\times .8,.364\\times .1)=.04$|$.1\\max(.1\\times .2,.364\\times .9)=.032$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough, we stop here. Until this point, our hidden sequence was FFF. From here on we should really write the algorithm. I have written one (you'll do it as a part of the homework), and the output I get is:\n",
    "\n",
    "```\n",
    "Observed oo--oooo-o\n",
    "Decode   FFFLLLLLLL\n",
    "p=7.31e-05\n",
    "```\n",
    "\n",
    "So it turns out that magician first uses a fair dice but then switches to the loaded one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage it is great to do some more programming. Say, we build a program that considers the hidden Markov model, generates the sequence of hidden states and emission, and then tries to decode the hidden states using Viterbi algorithm. I wrote one, and here is an example of one run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Hidden   FFFFFFFFFFFFFFFFFLLLLFFFFFFLLLLLLLFFLLLLLLLLLFFFFFLLLLLLLLLLLLLLLLFLLLLLLLLLLLLLLLLFFFFFFFFFFFFFFLLF\n",
    "Observed -o--ooo-o--o-o---ooooooo-ooooo-oooooooooooo-oo-o--oooooooooooooooo-oooooooo-ooooooooo-o-oo--o-ooooo-\n",
    "Decode   FFFFFFFFFFFFFFFFLLLLLLLLLLLLLLLLLLLLLLLLLLFFFFFFFLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLFFFFFFFFFLLLLLFF\n",
    "p=9.81e-29\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that decoded sequence is not exactly equal to the original sequence of hidden states. Why is that? Also, we can note taht the joint probability of the decoded sequence and emissions is rather very very low. Again, why? And how is it possible that our decoded sequence has higher joint probability that joint probability of emissions and original path? In fact, what is the joint probability of the path and emissions? Can you compute one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Algorithm, This Time More Formally\n",
    "\n",
    "Initialization:\n",
    "$v_0(0)=1$, $v_k(0)=0$ for all $k$\n",
    "\n",
    "Recursion ($i=1\\ldots L$):\n",
    "$$v_l(i)=e_l(x_i)\\max_k(v_k(i-1)a_{kl})$$\n",
    "$${\\rm ptr}_i(l)={\\rm argmax}_k(v_k(i-1)a_{kl})$$\n",
    "\n",
    "Termination:\n",
    "$$P(x, \\pi^*)=\\max_k(v_k(L)a_{k0})$$\n",
    "$$\\pi^*_L={\\rm argmax}_k(v_k(L)a_{k0})$$\n",
    "\n",
    "Traceback ($i=L\\ldots 1$):\n",
    "$$\\pi^*_{i-1}={\\rm ptr}_i(\\pi^*)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
